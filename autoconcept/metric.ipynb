{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import load_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching configuration...\n",
      "Loading datamodule...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/danis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/home/danis/anaconda3/envs/bottleneck/lib/python3.10/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 9609.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of vocab:  54\n",
      "Max len of caption:  16\n",
      "Index for <pad>: [0]\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danis/anaconda3/envs/bottleneck/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "/home/danis/anaconda3/envs/bottleneck/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'criterion_task' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion_task'])`.\n",
      "  rank_zero_warn(\n",
      "/home/danis/anaconda3/envs/bottleneck/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'criterion_tie' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion_tie'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# baseline /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-11/18-14-45\n",
    "# framework (simple) /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-11/19-47-58\n",
    "# framework /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-11/20-17-31\n",
    "# framework (parallel) /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-11/21-18-11\n",
    "# baseline (sigmoid) /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-12/06-58-58\n",
    "# framework with pre-trianing: /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-12/12-19-17\n",
    "\n",
    "# /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-15/11-14-43\n",
    "# /home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-15/12-44-33\n",
    "\n",
    "dm, model = load_experiment(\"/home/danis/Projects/AlphaCaption/AutoConceptBottleneck/autoconcept/outputs/2023-05-15/12-44-33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "train_set = train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [red, green, blue, square, triangle, circle]\n",
    "\n",
    "attribute_mapping = {\n",
    "    0: [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        ],\n",
    "    1: [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0\n",
    "        ],\n",
    "    2: [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0\n",
    "        ],\n",
    "    3: [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\n",
    "        ],\n",
    "    4: [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0\n",
    "        ],\n",
    "    5: [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0\n",
    "        ],\n",
    "    6: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0\n",
    "        ],\n",
    "    7: [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0\n",
    "        ],\n",
    "    8: [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-3.1333, device='cuda:0', grad_fn=<MinBackward1>), tensor(-3.2442, device='cuda:0', grad_fn=<MinBackward1>)) (tensor(1.6126, device='cuda:0', grad_fn=<MaxBackward1>), tensor(1.4456, device='cuda:0', grad_fn=<MaxBackward1>)) (tensor(0.4419, device='cuda:0', grad_fn=<MinBackward1>), tensor(0.0130, device='cuda:0', grad_fn=<MinBackward1>))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 1., 1., 1., 0., 0., 0., 1., 0.]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[1., 0., 0., 1., 1., 0., 1., 1., 1., 1.]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ = batch[\"image\"][0].cuda().unsqueeze(0)\n",
    "ind_ = batch[\"indices\"][0].cuda().unsqueeze(0)\n",
    "\n",
    "a = model(img_, ind_)\n",
    "\n",
    "a[\"feature_activated\"], a[\"concept_activated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_logits': tensor([[ 1.6126, -0.4419,  0.7730,  1.5096,  1.5260, -1.7645, -3.1333, -1.6441,\n",
       "           0.9579, -1.2017]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " 'concept_logits': tensor([[ 1.4456, -0.6435, -0.9513,  1.3009,  1.0171, -1.8065, -3.2442,  0.0130,\n",
       "          -0.1630, -0.4674]], device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " 'feature_probs': tensor([[0.8338, 0.3913, 0.6842, 0.8190, 0.8214, 0.1462, 0.0418, 0.1619, 0.7227,\n",
       "          0.2312]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " 'concept_probs': tensor([[0.8093, 0.3444, 0.2786, 0.7860, 0.7344, 0.1411, 0.0375, 0.5033, 0.4594,\n",
       "          0.3852]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " 'feature_activated': tensor([[1., 0., 1., 1., 1., 0., 0., 0., 1., 0.]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'concept_activated': tensor([[1., 0., 0., 1., 1., 0., 1., 1., 1., 1.]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'prediction': tensor([[-0.2330, -0.5872,  0.2372, -0.9621,  0.0871, -0.4125,  0.3145, -0.5565,\n",
       "           2.9055]], device='cuda:0', grad_fn=<NativeBatchNormBackward0>),\n",
       " 'scores': tensor([[[2.8698e-02, 7.3281e-03, 2.1249e-03, 5.7494e-03, 1.3515e-02,\n",
       "           1.1621e-01, 1.8468e-02, 2.4646e-03, 3.4690e-02, 7.7795e-03,\n",
       "           2.2354e-02, 1.7104e-02, 1.8921e-02, 1.3510e-07, 1.3510e-07,\n",
       "           1.3510e-07],\n",
       "          [4.4170e-02, 6.1644e-02, 1.5440e-02, 3.0976e-02, 7.9543e-02,\n",
       "           5.2748e-02, 3.0011e-02, 2.7984e-02, 2.3708e-02, 2.4959e-02,\n",
       "           9.7300e-02, 1.0021e-01, 5.5555e-02, 6.9911e-08, 6.9911e-08,\n",
       "           6.9911e-08],\n",
       "          [2.1388e-02, 5.8636e-02, 1.9924e-01, 1.5295e-01, 5.9067e-02,\n",
       "           2.4262e-02, 5.1601e-03, 5.8132e-02, 3.1016e-02, 4.6290e-02,\n",
       "           5.4500e-02, 6.1244e-02, 3.0074e-02, 4.0439e-08, 4.0439e-08,\n",
       "           4.0439e-08],\n",
       "          [4.1582e-02, 3.7379e-02, 5.1893e-02, 8.1154e-03, 1.7195e-01,\n",
       "           3.3738e-02, 2.4005e-03, 2.3997e-02, 6.6938e-03, 2.0021e-02,\n",
       "           3.4848e-02, 3.5215e-02, 5.4326e-02, 9.2726e-08, 9.2726e-08,\n",
       "           9.2726e-08],\n",
       "          [4.0342e-02, 3.2310e-02, 9.3664e-02, 2.5477e-02, 1.0634e-01,\n",
       "           4.8966e-02, 3.6477e-03, 2.6185e-02, 1.7908e-02, 3.3682e-02,\n",
       "           4.0554e-02, 4.0464e-02, 4.8360e-02, 8.6047e-08, 8.6047e-08,\n",
       "           8.6047e-08],\n",
       "          [2.5000e-02, 2.4422e-02, 1.0287e-02, 3.8252e-03, 1.4232e-01,\n",
       "           3.2458e-02, 4.5654e-03, 7.0050e-03, 5.5283e-03, 1.0068e-02,\n",
       "           3.1325e-02, 2.8495e-02, 4.7188e-02, 1.2070e-07, 1.2070e-07,\n",
       "           1.2070e-07],\n",
       "          [2.8173e-02, 1.4909e-02, 3.2525e-03, 1.3868e-03, 1.7862e-01,\n",
       "           4.9510e-02, 5.8802e-03, 5.1477e-03, 3.4791e-03, 7.0776e-03,\n",
       "           4.4409e-02, 2.7964e-02, 4.2259e-02, 1.1330e-07, 1.1330e-07,\n",
       "           1.1330e-07],\n",
       "          [3.9497e-02, 2.9971e-02, 7.4246e-04, 2.8401e-02, 8.1116e-03,\n",
       "           1.0520e-01, 2.8288e-01, 1.4446e-02, 5.8651e-02, 9.5744e-03,\n",
       "           1.3205e-01, 1.0438e-01, 1.6144e-02, 3.5192e-08, 3.5192e-08,\n",
       "           3.5192e-08],\n",
       "          [9.2632e-02, 7.6371e-02, 2.1013e-02, 7.9612e-02, 1.3692e-02,\n",
       "           6.5834e-02, 3.4206e-02, 7.0553e-02, 1.1682e-01, 7.0498e-02,\n",
       "           4.4728e-02, 6.2324e-02, 6.6992e-02, 3.7951e-08, 3.7951e-08,\n",
       "           3.7951e-08],\n",
       "          [1.5170e-02, 4.3939e-03, 1.3453e-03, 7.8425e-04, 5.3479e-02,\n",
       "           5.4663e-02, 2.9719e-03, 1.3794e-03, 4.9725e-03, 4.0046e-03,\n",
       "           1.7137e-02, 8.7905e-03, 1.9799e-02, 1.5501e-07, 1.5501e-07,\n",
       "           1.5501e-07],\n",
       "          [9.3828e-02, 9.4255e-02, 6.5052e-02, 6.2542e-02, 1.3564e-02,\n",
       "           2.7944e-02, 4.9573e-03, 1.3577e-01, 9.1497e-02, 1.4447e-01,\n",
       "           1.4499e-02, 2.4227e-02, 1.0598e-01, 2.6116e-08, 2.6116e-08,\n",
       "           2.6116e-08]]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " 'scores_aux': tensor([[[2.8698e-02, 7.3281e-03, 2.1249e-03, 5.7494e-03, 1.3515e-02,\n",
       "           1.1621e-01, 1.8468e-02, 2.4646e-03, 3.4690e-02, 7.7795e-03,\n",
       "           2.2354e-02, 1.7104e-02, 1.8921e-02, 1.3510e-07, 1.3510e-07,\n",
       "           1.3510e-07, 7.0460e-01],\n",
       "          [4.4170e-02, 6.1644e-02, 1.5440e-02, 3.0976e-02, 7.9543e-02,\n",
       "           5.2748e-02, 3.0011e-02, 2.7984e-02, 2.3708e-02, 2.4959e-02,\n",
       "           9.7300e-02, 1.0021e-01, 5.5555e-02, 6.9911e-08, 6.9911e-08,\n",
       "           6.9911e-08, 3.5575e-01],\n",
       "          [2.1388e-02, 5.8636e-02, 1.9924e-01, 1.5295e-01, 5.9067e-02,\n",
       "           2.4262e-02, 5.1601e-03, 5.8132e-02, 3.1016e-02, 4.6290e-02,\n",
       "           5.4500e-02, 6.1244e-02, 3.0074e-02, 4.0439e-08, 4.0439e-08,\n",
       "           4.0439e-08, 1.9804e-01],\n",
       "          [4.1582e-02, 3.7379e-02, 5.1893e-02, 8.1154e-03, 1.7195e-01,\n",
       "           3.3738e-02, 2.4005e-03, 2.3997e-02, 6.6938e-03, 2.0021e-02,\n",
       "           3.4848e-02, 3.5215e-02, 5.4326e-02, 9.2726e-08, 9.2726e-08,\n",
       "           9.2726e-08, 4.7784e-01],\n",
       "          [4.0342e-02, 3.2310e-02, 9.3664e-02, 2.5477e-02, 1.0634e-01,\n",
       "           4.8966e-02, 3.6477e-03, 2.6185e-02, 1.7908e-02, 3.3682e-02,\n",
       "           4.0554e-02, 4.0464e-02, 4.8360e-02, 8.6047e-08, 8.6047e-08,\n",
       "           8.6047e-08, 4.4210e-01],\n",
       "          [2.5000e-02, 2.4422e-02, 1.0287e-02, 3.8252e-03, 1.4232e-01,\n",
       "           3.2458e-02, 4.5654e-03, 7.0050e-03, 5.5283e-03, 1.0068e-02,\n",
       "           3.1325e-02, 2.8495e-02, 4.7188e-02, 1.2070e-07, 1.2070e-07,\n",
       "           1.2070e-07, 6.2751e-01],\n",
       "          [2.8173e-02, 1.4909e-02, 3.2525e-03, 1.3868e-03, 1.7862e-01,\n",
       "           4.9510e-02, 5.8802e-03, 5.1477e-03, 3.4791e-03, 7.0776e-03,\n",
       "           4.4409e-02, 2.7964e-02, 4.2259e-02, 1.1330e-07, 1.1330e-07,\n",
       "           1.1330e-07, 5.8793e-01],\n",
       "          [3.9497e-02, 2.9971e-02, 7.4246e-04, 2.8401e-02, 8.1116e-03,\n",
       "           1.0520e-01, 2.8288e-01, 1.4446e-02, 5.8651e-02, 9.5744e-03,\n",
       "           1.3205e-01, 1.0438e-01, 1.6144e-02, 3.5192e-08, 3.5192e-08,\n",
       "           3.5192e-08, 1.6996e-01],\n",
       "          [9.2632e-02, 7.6371e-02, 2.1013e-02, 7.9612e-02, 1.3692e-02,\n",
       "           6.5834e-02, 3.4206e-02, 7.0553e-02, 1.1682e-01, 7.0498e-02,\n",
       "           4.4728e-02, 6.2324e-02, 6.6992e-02, 3.7951e-08, 3.7951e-08,\n",
       "           3.7951e-08, 1.8472e-01],\n",
       "          [1.5170e-02, 4.3939e-03, 1.3453e-03, 7.8425e-04, 5.3479e-02,\n",
       "           5.4663e-02, 2.9719e-03, 1.3794e-03, 4.9725e-03, 4.0046e-03,\n",
       "           1.7137e-02, 8.7905e-03, 1.9799e-02, 1.5501e-07, 1.5501e-07,\n",
       "           1.5501e-07, 8.1111e-01],\n",
       "          [9.3828e-02, 9.4255e-02, 6.5052e-02, 6.2542e-02, 1.3564e-02,\n",
       "           2.7944e-02, 4.9573e-03, 1.3577e-01, 9.1497e-02, 1.4447e-01,\n",
       "           1.4499e-02, 2.4227e-02, 1.0598e-01, 2.6116e-08, 2.6116e-08,\n",
       "           2.6116e-08, 1.2141e-01]]], device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'prediction_aux': tensor([[-1.3432, -0.9188, -0.5857, -0.3053,  0.5585,  0.1630,  0.4517,  1.5145,\n",
       "           1.3738]], device='cuda:0', grad_fn=<NativeBatchNormBackward0>),\n",
       " 'loss_dist': tensor(0.1655, device='cuda:0', grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7029143 , 0.00566914, 0.88424534, 0.05497954, 0.02836036,\n",
       "       0.7826127 , 0.9160332 , 0.58906835, 0.70632035, 0.0124381 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_set[50]\n",
    "image = sample[\"image\"]\n",
    "captions = sample[\"report\"]\n",
    "\n",
    "image = image.unsqueeze(0).cuda()\n",
    "# captions = captions.unsqueeze(0).cuda()\n",
    "\n",
    "\n",
    "model.main.inference(image)[1].cpu().detach().numpy()[0]\n",
    "# model(image)[\"concept_probs\"].cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LitAutoConceptBottleneckModel.forward() missing 1 required positional argument: 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(image)\n",
      "File \u001b[0;32m~/anaconda3/envs/bottleneck/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: LitAutoConceptBottleneckModel.forward() missing 1 required positional argument: 'indices'"
     ]
    }
   ],
   "source": [
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0303, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.2492, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "Parameter containing:\n",
      "tensor(1.0729, device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor(0.3637, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'feature_logits': tensor([[ 1.1863, -4.5293,  1.4814, -2.2510, -4.0154,  1.1285,  1.5703, -0.6106,\n",
       "           1.1538, -2.9983]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " 'concept_logits': tensor([[-0.2064, -0.3806, -0.6131, -0.3321,  0.9779, -0.7432, -0.4473,  0.9430,\n",
       "           1.1664, -2.3901]], device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " 'feature_probs': tensor([[0.7661, 0.0107, 0.8148, 0.0953, 0.0177, 0.7556, 0.8278, 0.3519, 0.7602,\n",
       "          0.0475]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " 'concept_probs': tensor([[0.4486, 0.4060, 0.3513, 0.4177, 0.7267, 0.3223, 0.3900, 0.7197, 0.7625,\n",
       "          0.0839]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " 'feature_activated': tensor([[1., 0., 1., 0., 0., 1., 1., 1., 1., 0.]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'prediction': tensor([[ 2.4578,  0.1548,  0.3244, -0.5086, -0.1632, -1.1643,  0.1855, -0.4218,\n",
       "          -0.6592]], device='cuda:0', grad_fn=<NativeBatchNormBackward0>),\n",
       " 'scores': tensor([[[1.3821e-07, 2.0446e-01, 1.3821e-07],\n",
       "          [1.6603e-07, 2.3523e-02, 1.5759e-03],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07],\n",
       "          [1.2330e-07, 3.0059e-01, 1.2330e-07],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07],\n",
       "          [1.1819e-07, 1.8580e-01, 1.4782e-01],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07],\n",
       "          [8.4225e-08, 2.0952e-01, 3.4351e-01],\n",
       "          [1.6975e-07, 1.1226e-03, 1.6975e-07],\n",
       "          [8.4420e-08, 1.5826e-01, 3.9351e-01]]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward0>),\n",
       " 'scores_aux': tensor([[[1.3821e-07, 2.0446e-01, 1.3821e-07, 7.9554e-01],\n",
       "          [1.6603e-07, 2.3523e-02, 1.5759e-03, 9.7490e-01],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07, 1.0000e+00],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07, 1.0000e+00],\n",
       "          [1.2330e-07, 3.0059e-01, 1.2330e-07, 6.9941e-01],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07, 1.0000e+00],\n",
       "          [1.1819e-07, 1.8580e-01, 1.4782e-01, 6.6638e-01],\n",
       "          [1.6993e-07, 1.6993e-07, 1.6993e-07, 1.0000e+00],\n",
       "          [8.4225e-08, 2.0952e-01, 3.4351e-01, 4.4697e-01],\n",
       "          [1.6975e-07, 1.1226e-03, 1.6975e-07, 9.9888e-01],\n",
       "          [8.4420e-08, 1.5826e-01, 3.9351e-01, 4.4823e-01]]], device='cuda:0',\n",
       "        grad_fn=<DivBackward0>),\n",
       " 'prediction_aux': tensor([[-0.6735, -0.5141,  1.0488,  0.1143, -0.0404,  1.7420, -1.2569, -0.5857,\n",
       "           0.3594]], device='cuda:0', grad_fn=<NativeBatchNormBackward0>),\n",
       " 'loss_dist': tensor(-0.1045, device='cuda:0', grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image, torch.tensor([[0, 10, 1]]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1845/1845 [00:40<00:00, 45.65it/s]\n",
      "100%|██████████| 1845/1845 [00:42<00:00, 43.85it/s]\n",
      "100%|██████████| 1845/1845 [00:38<00:00, 48.08it/s]\n",
      "100%|██████████| 1845/1845 [00:38<00:00, 48.02it/s]\n",
      "100%|██████████| 1845/1845 [00:38<00:00, 48.13it/s]\n",
      "100%|██████████| 1845/1845 [00:38<00:00, 48.39it/s]\n",
      "100%|██████████| 1845/1845 [00:37<00:00, 48.99it/s]\n",
      "100%|██████████| 1845/1845 [00:37<00:00, 48.82it/s]\n",
      "100%|██████████| 1845/1845 [00:37<00:00, 48.74it/s]\n",
      "100%|██████████| 1845/1845 [00:38<00:00, 48.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7367255094381213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "\n",
    "for feature_id in range(10):\n",
    "\n",
    "    attribute_values = [[0, 0] for _ in range(len(attribute_mapping[0]))]\n",
    "\n",
    "    for i in tqdm(range(len(train_set))):\n",
    "        sample = train_set[i]\n",
    "        \n",
    "        idx = sample[\"target\"].item()\n",
    "        attributes = np.array(attribute_mapping[idx])\n",
    "\n",
    "        image = sample[\"image\"]\n",
    "        image = image.unsqueeze(0).cuda()\n",
    "\n",
    "        feature = model.main.inference(image)[1].cpu().detach().numpy()[0][feature_id]\n",
    "        # feature = model(image)[\"concept_probs\"].cpu().detach().numpy()[0][feature_id]\n",
    "\n",
    "        for attr_id, attribute in enumerate(attributes):\n",
    "            value_on = attribute * feature + (1 - attribute) * (1 - feature)\n",
    "            attribute_values[attr_id][0] += value_on\n",
    "\n",
    "            value_off = attribute * (1 - feature) + (1 - attribute) * feature\n",
    "            attribute_values[attr_id][1] += value_off\n",
    "    \n",
    "    attribute_values_max = [max(p) for p in attribute_values]\n",
    "    attribute_values_max = max(attribute_values_max)\n",
    "    score = attribute_values_max / len(train_set)\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "metric = np.array(scores).mean()\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: 0.7722590434662505\n",
    "\n",
    "Framework (simplify): 0.7840587777528067\n",
    "\n",
    "Framework: 0.7172063330401222, 0.7068478698766005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: 0.7481238945043126 / 0.8002787673469156 / \n",
    "\n",
    "Our framework: 0.733698356318566 / 0.733698356318566 / 0.77456 (w/o textual, anneal)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8573726896832234,\n",
       "  0.839452540636915,\n",
       "  0.7797192060542814,\n",
       "  0.8681423352541888,\n",
       "  0.8260357244465,\n",
       "  0.8895389913930329,\n",
       "  0.8637059218046688,\n",
       "  0.7592228388927755,\n",
       "  0.797043976947101,\n",
       "  0.7546556928753102],\n",
       " 0.8234889917987998)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, np.array(scores).mean() # baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7648627196583573,\n",
       " 0.6768948672750362,\n",
       " 0.7658918708248464,\n",
       " 0.9917780249794046,\n",
       " 0.8336742381045732,\n",
       " 0.7635008184147333,\n",
       " 0.7613273321342036,\n",
       " 0.7518359585674752,\n",
       " 0.6415101127414388,\n",
       " 0.7674385555471499]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[ 0.0122, -0.0529, -0.0110,  ..., -0.0344, -0.0533,  0.0093],\n",
    "        [ 0.0010,  0.0120,  0.0526,  ..., -0.0378,  0.0056, -0.0115],\n",
    "        [-0.0123,  0.0306,  0.0034,  ..., -0.0043, -0.0447,  0.0298],\n",
    "        ...,\n",
    "        [ 0.0362,  0.0370, -0.0468,  ...,  0.0215,  0.0100,  0.0521],\n",
    "        [ 0.0442,  0.0317, -0.0362,  ...,  0.0499, -0.0278,  0.0430],\n",
    "        [ 0.0372, -0.0121,  0.0365,  ..., -0.0190, -0.0329, -0.0160]],"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor:  Parameter containing:\n",
    "tensor([[-0.0137, -0.0356,  0.0175,  ..., -0.0315,  0.0064, -0.0237],\n",
    "        [-0.0178,  0.0340,  0.0079,  ...,  0.0032, -0.0259, -0.0187],\n",
    "        [ 0.0283, -0.0175, -0.0104,  ..., -0.0227, -0.0323, -0.0349],\n",
    "        ...,\n",
    "        [-0.0439,  0.0097, -0.0294,  ..., -0.0208,  0.0355,  0.0253],\n",
    "        [-0.0421, -0.0303, -0.0202,  ..., -0.0343,  0.0074,  0.0075],\n",
    "        [ 0.0195,  0.0237, -0.0137,  ..., -0.0427, -0.0188,  0.0273]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "isinstance(torch.nn.Linear(10, 5), torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bottleneck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
