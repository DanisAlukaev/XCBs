_target_: models.auto_concept_bottleneck.LitAutoConceptBottleneckModel

main:
  _target_: models.auto_concept_bottleneck.AutoConceptBottleneckModel

  feature_extractor:
    _target_: models.feature_extractors.torchvision.TorchvisionFeatureExtractor
    model: inception_v3
    weights: null # IMAGENET1K_V1 # null
    out_features: 10

  concept_extractor:
    _target_: models.concept_extractors.novel.ConceptExtractorAttention
    vocab_size: 55
    embed_dim: 8
    out_features: 10
    max_len: 30
    src_pad_idx: 0
    dropout: 0.
    use_slot_norm: true
    norm_fn1:
      _target_: functional.activation.Entmax15
      # _target_: torch.nn.Softmax
      dim: -2
    norm_fn2:
      _target_: functional.operations.ScalerSum
      dim: -1
    regularize_distance: false
    use_position_encoding: true
    mlp_depth: 4
    use_dummy_attention: true
    activation:
      _target_: torch.nn.ReLU
    device: cuda

  predictor:
    _target_: models.predictors.mlp.MLPPredictor
    layers: [10, 9]
    activation:
      _target_: torch.nn.ReLU

  interim_activation:
    # _target_: torch.nn.Sigmoid
    _target_: functional.gumbel.GumbelSigmoid
    step: 15
    t: 1.0
    rate: 0.0035
    min_val: 0.001

  predictor_aux:
    _target_: models.predictors.mlp.MLPPredictor
    layers: [10, 9]
    activation:
      _target_: torch.nn.ReLU

dist_weight: 0.0

criterion_task:
  _target_: torch.nn.CrossEntropyLoss

criterion_tie:
  _target_: functional.loss.JensenShannonDivergenceLoss
  # _target_: functional.loss.KullbackLeiblerDivergenceLoss
  # _target_: torch.nn.L1Loss
tie_weight: 10.0

mix_tie_epoch: null

pretrain_embeddings_epoch: null

tie_loss_wrt_concepts: true

optimizer_model_template:
  _target_: functools.partial
  _args_:
    - _target_: hydra.utils.get_class
      path: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.02

optimizer_concept_extractor_template:
  _target_: functools.partial
  _args_:
    - _target_: hydra.utils.get_class
      path: torch.optim.AdamW
  lr: 6e-3
  # lr: 0.001
  weight_decay: 0.03

scheduler_model_template:
  _target_: functools.partial
  _args_:
    - _target_: hydra.utils.get_class
      path: torch.optim.lr_scheduler.MultiStepLR
  milestones: [5, 7]
  gamma: 0.1

scheduler_concept_extractor_template:
  _target_: functools.partial
  _args_:
    - _target_: hydra.utils.get_class
      path: torch.optim.lr_scheduler.MultiStepLR
  milestones: [6, 8]
  gamma: 0.3
