{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOKENS = 500\n",
    "N_CONCEPTS = 300\n",
    "EMBED_SIZE = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.rand(BATCH_SIZE, N_TOKENS, EMBED_SIZE)\n",
    "concepts_ = torch.rand(BATCH_SIZE, N_CONCEPTS, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = nn.MultiheadAttention(embed_dim=EMBED_SIZE, num_heads=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 300, 100]), torch.Size([64, 300, 500]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, weights = attention(concepts_, input_, input_)\n",
    "\n",
    "out.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights.sum(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(64, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(20, 20)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5392, -1.6842,  2.1287, -0.9996, -0.0918, -1.0289,  0.7269, -0.8552,\n",
       "         1.4781, -2.0916,  1.4071, -0.0483,  1.4157, -1.1046, -0.7982, -0.0397,\n",
       "        -1.3845,  0.2805,  0.3128,  0.2121, -0.3989,  0.2836,  1.6595, -1.7762,\n",
       "        -0.1208,  0.8577, -0.0425, -0.3098, -0.6341, -0.6159, -0.7735,  0.8514,\n",
       "        -0.6448, -0.0686,  0.1370,  0.2293, -0.0859, -2.0357,  1.0383, -1.3259,\n",
       "         0.5748, -0.0483,  0.0436, -1.6986, -1.4176, -1.2189,  0.4861,  0.4967,\n",
       "        -0.9279, -0.8002, -0.0323, -2.8358,  0.1157, -1.4193, -0.6376,  0.2255,\n",
       "         0.3400, -0.8765,  0.7401, -0.7537,  1.8084, -0.8462, -1.8286, -1.9487,\n",
       "        -0.8952, -0.5999, -1.1553, -1.7724,  0.4262, -0.7032,  0.6454, -0.5208,\n",
       "         0.6742, -0.6968,  0.2483, -0.3675, -0.8308,  0.5099, -2.0244, -0.7917,\n",
       "         0.9039,  1.0445, -0.1016, -0.1148, -1.8705, -0.0868,  0.5005, -0.9067,\n",
       "         1.4955, -1.4520, -0.2804,  0.9535, -0.6732,  0.9717, -0.0445,  0.8808,\n",
       "         2.0937, -0.5675, -1.3792, -0.6508], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Embedding(10, 100)(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 30\n",
    "N_CONCEPTS = 5\n",
    "\n",
    "query_w = nn.Embedding(N_CONCEPTS + 1, EMBED_DIM)\n",
    "keys_w = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "\n",
    "input_embedding = torch.rand((BATCH_SIZE, SEQ_LEN, EMBED_DIM))\n",
    "keys = keys_w(input_embedding)\n",
    "values = keys_w(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 100]), torch.Size([64, 30, 100]), torch.Size([64, 30, 100]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_w.weight.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 6, 30]), torch.Size([64, 6, 100]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.matmul(query_w.weight, keys.transpose(-2, -1))\n",
    "m = F.softmax(x  / (EMBED_DIM ** (1 /2)), dim=-2)\n",
    "a = m / m.sum(dim=-1, keepdim=True)\n",
    "out = torch.matmul(a, keys)\n",
    "a.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0328, 0.0205, 0.0339,  ..., 0.0305, 0.0377, 0.0441],\n",
       "         [0.0392, 0.0373, 0.0308,  ..., 0.0382, 0.0297, 0.0319],\n",
       "         [0.0435, 0.0491, 0.0309,  ..., 0.0300, 0.0297, 0.0296],\n",
       "         [0.0330, 0.0383, 0.0341,  ..., 0.0374, 0.0334, 0.0322],\n",
       "         [0.0330, 0.0388, 0.0365,  ..., 0.0336, 0.0404, 0.0237],\n",
       "         [0.0296, 0.0318, 0.0320,  ..., 0.0324, 0.0278, 0.0318]],\n",
       "\n",
       "        [[0.0286, 0.0208, 0.0340,  ..., 0.0407, 0.0396, 0.0294],\n",
       "         [0.0322, 0.0259, 0.0334,  ..., 0.0371, 0.0193, 0.0360],\n",
       "         [0.0292, 0.0244, 0.0392,  ..., 0.0379, 0.0232, 0.0380],\n",
       "         [0.0441, 0.0380, 0.0316,  ..., 0.0309, 0.0334, 0.0366],\n",
       "         [0.0335, 0.0439, 0.0366,  ..., 0.0305, 0.0296, 0.0387],\n",
       "         [0.0309, 0.0393, 0.0305,  ..., 0.0282, 0.0364, 0.0293]],\n",
       "\n",
       "        [[0.0309, 0.0275, 0.0368,  ..., 0.0333, 0.0274, 0.0331],\n",
       "         [0.0294, 0.0314, 0.0443,  ..., 0.0384, 0.0236, 0.0323],\n",
       "         [0.0326, 0.0306, 0.0393,  ..., 0.0329, 0.0253, 0.0319],\n",
       "         [0.0286, 0.0307, 0.0298,  ..., 0.0354, 0.0461, 0.0345],\n",
       "         [0.0267, 0.0367, 0.0385,  ..., 0.0291, 0.0294, 0.0256],\n",
       "         [0.0443, 0.0402, 0.0249,  ..., 0.0330, 0.0367, 0.0379]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0302, 0.0338, 0.0329,  ..., 0.0313, 0.0229, 0.0288],\n",
       "         [0.0335, 0.0292, 0.0315,  ..., 0.0347, 0.0342, 0.0371],\n",
       "         [0.0447, 0.0292, 0.0359,  ..., 0.0382, 0.0319, 0.0389],\n",
       "         [0.0308, 0.0337, 0.0293,  ..., 0.0354, 0.0361, 0.0271],\n",
       "         [0.0345, 0.0278, 0.0248,  ..., 0.0430, 0.0362, 0.0436],\n",
       "         [0.0334, 0.0380, 0.0408,  ..., 0.0264, 0.0380, 0.0329]],\n",
       "\n",
       "        [[0.0349, 0.0404, 0.0338,  ..., 0.0340, 0.0402, 0.0275],\n",
       "         [0.0308, 0.0304, 0.0325,  ..., 0.0300, 0.0349, 0.0338],\n",
       "         [0.0333, 0.0360, 0.0376,  ..., 0.0275, 0.0410, 0.0341],\n",
       "         [0.0349, 0.0320, 0.0395,  ..., 0.0364, 0.0270, 0.0379],\n",
       "         [0.0315, 0.0281, 0.0306,  ..., 0.0292, 0.0238, 0.0399],\n",
       "         [0.0326, 0.0310, 0.0289,  ..., 0.0358, 0.0343, 0.0310]],\n",
       "\n",
       "        [[0.0351, 0.0366, 0.0426,  ..., 0.0401, 0.0417, 0.0453],\n",
       "         [0.0293, 0.0268, 0.0278,  ..., 0.0313, 0.0262, 0.0303],\n",
       "         [0.0316, 0.0244, 0.0340,  ..., 0.0353, 0.0351, 0.0299],\n",
       "         [0.0407, 0.0359, 0.0287,  ..., 0.0320, 0.0287, 0.0248],\n",
       "         [0.0313, 0.0337, 0.0369,  ..., 0.0340, 0.0252, 0.0296],\n",
       "         [0.0290, 0.0327, 0.0278,  ..., 0.0278, 0.0349, 0.0327]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.8850],\n",
       "         [2.1320],\n",
       "         [2.4652],\n",
       "         [5.8061],\n",
       "         [4.4785],\n",
       "         [8.2332]],\n",
       "\n",
       "        [[6.9114],\n",
       "         [2.0397],\n",
       "         [2.4101],\n",
       "         [6.0034],\n",
       "         [4.4468],\n",
       "         [8.1886]],\n",
       "\n",
       "        [[7.1254],\n",
       "         [2.0725],\n",
       "         [2.6072],\n",
       "         [5.8964],\n",
       "         [4.5390],\n",
       "         [7.7596]],\n",
       "\n",
       "        [[7.0380],\n",
       "         [2.2076],\n",
       "         [2.6020],\n",
       "         [5.5359],\n",
       "         [4.3267],\n",
       "         [8.2897]],\n",
       "\n",
       "        [[6.9540],\n",
       "         [1.9268],\n",
       "         [2.5414],\n",
       "         [5.8026],\n",
       "         [4.5935],\n",
       "         [8.1817]],\n",
       "\n",
       "        [[6.4423],\n",
       "         [2.1320],\n",
       "         [2.6314],\n",
       "         [5.9470],\n",
       "         [4.8233],\n",
       "         [8.0239]],\n",
       "\n",
       "        [[6.4074],\n",
       "         [2.1084],\n",
       "         [2.6647],\n",
       "         [5.8869],\n",
       "         [4.5719],\n",
       "         [8.3607]],\n",
       "\n",
       "        [[6.6095],\n",
       "         [2.0606],\n",
       "         [2.5402],\n",
       "         [5.8911],\n",
       "         [4.5112],\n",
       "         [8.3873]],\n",
       "\n",
       "        [[6.6921],\n",
       "         [2.0790],\n",
       "         [2.4713],\n",
       "         [6.2103],\n",
       "         [4.5804],\n",
       "         [7.9669]],\n",
       "\n",
       "        [[6.6394],\n",
       "         [2.2159],\n",
       "         [2.6267],\n",
       "         [5.7365],\n",
       "         [4.5579],\n",
       "         [8.2236]],\n",
       "\n",
       "        [[7.0715],\n",
       "         [2.1685],\n",
       "         [2.5648],\n",
       "         [5.7683],\n",
       "         [4.3831],\n",
       "         [8.0437]],\n",
       "\n",
       "        [[6.3608],\n",
       "         [2.1442],\n",
       "         [2.7525],\n",
       "         [5.9541],\n",
       "         [4.5768],\n",
       "         [8.2115]],\n",
       "\n",
       "        [[6.5731],\n",
       "         [2.0551],\n",
       "         [2.4926],\n",
       "         [5.9122],\n",
       "         [4.5138],\n",
       "         [8.4532]],\n",
       "\n",
       "        [[6.6544],\n",
       "         [2.0826],\n",
       "         [2.6959],\n",
       "         [5.9360],\n",
       "         [4.5729],\n",
       "         [8.0582]],\n",
       "\n",
       "        [[6.9226],\n",
       "         [1.9757],\n",
       "         [2.5142],\n",
       "         [6.0413],\n",
       "         [4.6668],\n",
       "         [7.8794]],\n",
       "\n",
       "        [[6.7156],\n",
       "         [2.3262],\n",
       "         [2.6505],\n",
       "         [5.5392],\n",
       "         [4.5614],\n",
       "         [8.2072]],\n",
       "\n",
       "        [[6.7206],\n",
       "         [2.0505],\n",
       "         [2.5533],\n",
       "         [6.0250],\n",
       "         [4.5074],\n",
       "         [8.1430]],\n",
       "\n",
       "        [[6.5037],\n",
       "         [2.0425],\n",
       "         [2.6593],\n",
       "         [6.0385],\n",
       "         [4.6736],\n",
       "         [8.0823]],\n",
       "\n",
       "        [[6.9285],\n",
       "         [2.2135],\n",
       "         [2.5861],\n",
       "         [5.7858],\n",
       "         [4.4937],\n",
       "         [7.9925]],\n",
       "\n",
       "        [[6.8755],\n",
       "         [2.1499],\n",
       "         [2.6001],\n",
       "         [5.7205],\n",
       "         [4.5062],\n",
       "         [8.1479]],\n",
       "\n",
       "        [[6.8932],\n",
       "         [1.9740],\n",
       "         [2.5482],\n",
       "         [6.2960],\n",
       "         [4.4285],\n",
       "         [7.8601]],\n",
       "\n",
       "        [[6.9812],\n",
       "         [2.1025],\n",
       "         [2.5583],\n",
       "         [5.9895],\n",
       "         [4.6288],\n",
       "         [7.7397]],\n",
       "\n",
       "        [[6.6357],\n",
       "         [2.0481],\n",
       "         [2.6212],\n",
       "         [6.0044],\n",
       "         [4.6548],\n",
       "         [8.0357]],\n",
       "\n",
       "        [[7.1091],\n",
       "         [2.1379],\n",
       "         [2.4561],\n",
       "         [5.8110],\n",
       "         [4.3883],\n",
       "         [8.0977]],\n",
       "\n",
       "        [[6.6170],\n",
       "         [2.0998],\n",
       "         [2.6941],\n",
       "         [5.9359],\n",
       "         [4.7530],\n",
       "         [7.9002]],\n",
       "\n",
       "        [[6.4653],\n",
       "         [2.0262],\n",
       "         [2.5999],\n",
       "         [5.9109],\n",
       "         [4.6128],\n",
       "         [8.3850]],\n",
       "\n",
       "        [[6.5831],\n",
       "         [2.1240],\n",
       "         [2.6968],\n",
       "         [5.6973],\n",
       "         [4.6779],\n",
       "         [8.2209]],\n",
       "\n",
       "        [[6.6292],\n",
       "         [2.2396],\n",
       "         [2.5729],\n",
       "         [5.6808],\n",
       "         [4.3873],\n",
       "         [8.4902]],\n",
       "\n",
       "        [[6.6540],\n",
       "         [2.0938],\n",
       "         [2.5311],\n",
       "         [5.9848],\n",
       "         [4.4783],\n",
       "         [8.2580]],\n",
       "\n",
       "        [[6.5081],\n",
       "         [1.9930],\n",
       "         [2.6469],\n",
       "         [6.2179],\n",
       "         [4.5315],\n",
       "         [8.1026]],\n",
       "\n",
       "        [[6.9042],\n",
       "         [2.3197],\n",
       "         [2.8063],\n",
       "         [5.6577],\n",
       "         [4.5506],\n",
       "         [7.7615]],\n",
       "\n",
       "        [[6.6810],\n",
       "         [2.0957],\n",
       "         [2.6731],\n",
       "         [5.7424],\n",
       "         [4.5867],\n",
       "         [8.2210]],\n",
       "\n",
       "        [[6.4969],\n",
       "         [2.2495],\n",
       "         [2.5966],\n",
       "         [5.5936],\n",
       "         [4.8698],\n",
       "         [8.1936]],\n",
       "\n",
       "        [[7.0045],\n",
       "         [1.9855],\n",
       "         [2.7186],\n",
       "         [5.8523],\n",
       "         [4.4010],\n",
       "         [8.0381]],\n",
       "\n",
       "        [[6.7151],\n",
       "         [2.0718],\n",
       "         [2.5880],\n",
       "         [5.9678],\n",
       "         [4.5402],\n",
       "         [8.1171]],\n",
       "\n",
       "        [[6.6623],\n",
       "         [2.1435],\n",
       "         [2.5123],\n",
       "         [5.8932],\n",
       "         [4.5199],\n",
       "         [8.2688]],\n",
       "\n",
       "        [[6.9062],\n",
       "         [2.1796],\n",
       "         [2.6350],\n",
       "         [5.8381],\n",
       "         [4.3704],\n",
       "         [8.0707]],\n",
       "\n",
       "        [[6.5459],\n",
       "         [2.1725],\n",
       "         [2.5697],\n",
       "         [5.8364],\n",
       "         [4.6370],\n",
       "         [8.2385]],\n",
       "\n",
       "        [[6.7122],\n",
       "         [2.1332],\n",
       "         [2.4805],\n",
       "         [5.7790],\n",
       "         [4.6935],\n",
       "         [8.2017]],\n",
       "\n",
       "        [[7.0403],\n",
       "         [2.1654],\n",
       "         [2.4837],\n",
       "         [5.7543],\n",
       "         [4.7206],\n",
       "         [7.8358]],\n",
       "\n",
       "        [[6.5407],\n",
       "         [2.0859],\n",
       "         [2.5244],\n",
       "         [5.9899],\n",
       "         [4.6894],\n",
       "         [8.1698]],\n",
       "\n",
       "        [[6.9541],\n",
       "         [2.1252],\n",
       "         [2.5481],\n",
       "         [5.6571],\n",
       "         [4.6023],\n",
       "         [8.1132]],\n",
       "\n",
       "        [[6.5082],\n",
       "         [2.0877],\n",
       "         [2.5907],\n",
       "         [5.8151],\n",
       "         [4.7464],\n",
       "         [8.2519]],\n",
       "\n",
       "        [[7.0577],\n",
       "         [2.0734],\n",
       "         [2.5175],\n",
       "         [5.8479],\n",
       "         [4.6211],\n",
       "         [7.8824]],\n",
       "\n",
       "        [[6.8985],\n",
       "         [2.1288],\n",
       "         [2.6476],\n",
       "         [5.6902],\n",
       "         [4.6809],\n",
       "         [7.9540]],\n",
       "\n",
       "        [[6.6721],\n",
       "         [2.1343],\n",
       "         [2.6174],\n",
       "         [5.8834],\n",
       "         [4.6635],\n",
       "         [8.0293]],\n",
       "\n",
       "        [[7.0350],\n",
       "         [2.0471],\n",
       "         [2.5314],\n",
       "         [5.6994],\n",
       "         [4.5747],\n",
       "         [8.1124]],\n",
       "\n",
       "        [[6.7571],\n",
       "         [2.1931],\n",
       "         [2.4524],\n",
       "         [5.8161],\n",
       "         [4.5316],\n",
       "         [8.2498]],\n",
       "\n",
       "        [[6.8323],\n",
       "         [2.0852],\n",
       "         [2.6294],\n",
       "         [5.5480],\n",
       "         [4.4645],\n",
       "         [8.4406]],\n",
       "\n",
       "        [[6.7267],\n",
       "         [2.0911],\n",
       "         [2.6200],\n",
       "         [5.7627],\n",
       "         [4.7418],\n",
       "         [8.0578]],\n",
       "\n",
       "        [[6.6440],\n",
       "         [2.1512],\n",
       "         [2.6149],\n",
       "         [5.8230],\n",
       "         [4.5942],\n",
       "         [8.1726]],\n",
       "\n",
       "        [[6.7356],\n",
       "         [2.1995],\n",
       "         [2.6748],\n",
       "         [5.4984],\n",
       "         [4.5636],\n",
       "         [8.3282]],\n",
       "\n",
       "        [[6.9827],\n",
       "         [2.1291],\n",
       "         [2.5998],\n",
       "         [5.7022],\n",
       "         [4.8175],\n",
       "         [7.7687]],\n",
       "\n",
       "        [[6.5459],\n",
       "         [2.3494],\n",
       "         [2.7717],\n",
       "         [5.6595],\n",
       "         [4.7562],\n",
       "         [7.9174]],\n",
       "\n",
       "        [[6.5141],\n",
       "         [2.0178],\n",
       "         [2.4760],\n",
       "         [6.3059],\n",
       "         [4.4744],\n",
       "         [8.2117]],\n",
       "\n",
       "        [[6.9640],\n",
       "         [2.3010],\n",
       "         [2.6494],\n",
       "         [5.9000],\n",
       "         [4.4570],\n",
       "         [7.7287]],\n",
       "\n",
       "        [[6.7304],\n",
       "         [2.0898],\n",
       "         [2.5660],\n",
       "         [5.9294],\n",
       "         [4.6365],\n",
       "         [8.0480]],\n",
       "\n",
       "        [[6.7175],\n",
       "         [2.1457],\n",
       "         [2.6032],\n",
       "         [5.7337],\n",
       "         [4.5807],\n",
       "         [8.2192]],\n",
       "\n",
       "        [[6.6741],\n",
       "         [2.0562],\n",
       "         [2.5873],\n",
       "         [5.9770],\n",
       "         [4.6713],\n",
       "         [8.0341]],\n",
       "\n",
       "        [[6.8214],\n",
       "         [2.1531],\n",
       "         [2.5629],\n",
       "         [5.7095],\n",
       "         [4.3465],\n",
       "         [8.4066]],\n",
       "\n",
       "        [[6.8311],\n",
       "         [2.0274],\n",
       "         [2.4381],\n",
       "         [5.9565],\n",
       "         [4.4994],\n",
       "         [8.2476]],\n",
       "\n",
       "        [[6.4256],\n",
       "         [2.0450],\n",
       "         [2.4963],\n",
       "         [5.8665],\n",
       "         [4.6325],\n",
       "         [8.5341]],\n",
       "\n",
       "        [[6.9435],\n",
       "         [2.1505],\n",
       "         [2.7028],\n",
       "         [5.6054],\n",
       "         [4.6144],\n",
       "         [7.9833]],\n",
       "\n",
       "        [[7.1974],\n",
       "         [2.0368],\n",
       "         [2.4252],\n",
       "         [5.8729],\n",
       "         [4.3077],\n",
       "         [8.1599]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (6) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m \u001b[39m/\u001b[39;49m m\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (30) must match the size of tensor b (6) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    " m / m.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 30])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0413, 0.0517, 0.0329, 0.0268, 0.0346, 0.0383, 0.0256, 0.0314, 0.0227,\n",
       "        0.0369, 0.0412, 0.0261, 0.0311, 0.0406, 0.0363, 0.0346, 0.0344, 0.0329,\n",
       "        0.0268, 0.0274, 0.0363, 0.0286, 0.0327, 0.0277, 0.0456, 0.0318, 0.0312,\n",
       "        0.0313, 0.0320, 0.0291], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([10, 10, 5]) / np.array([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n_tokens = np.zeros(6)\n",
    "indices_np = np.array([0, 0, 4, 5])\n",
    "# n_tokens[indices_np]+= 1\n",
    "print(n_tokens[indices_np])\n",
    "\n",
    "scores_np = np.array([1, 1, 1, 1])\n",
    "scores_np_prev = np.array([0, 0, 0, 0])\n",
    "\n",
    "# np.put(n_tokens, indices_np, scores_np + scores_np_prev)\n",
    "print(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 0., 2., 2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens[indices_np] += scores_np\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.put(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_elem, index_np in enumerate(indices_np):\n",
    "    n_tokens[index_np] += scores_np[idx_elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bottleneck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
