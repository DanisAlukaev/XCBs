{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOKENS = 500\n",
    "N_CONCEPTS = 300\n",
    "EMBED_SIZE = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.rand(BATCH_SIZE, N_TOKENS, EMBED_SIZE)\n",
    "concepts_ = torch.rand(BATCH_SIZE, N_CONCEPTS, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = nn.MultiheadAttention(embed_dim=EMBED_SIZE, num_heads=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 300, 100]), torch.Size([64, 300, 500]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, weights = attention(concepts_, input_, input_)\n",
    "\n",
    "out.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights.sum(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(64, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(20, 20)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5392, -1.6842,  2.1287, -0.9996, -0.0918, -1.0289,  0.7269, -0.8552,\n",
       "         1.4781, -2.0916,  1.4071, -0.0483,  1.4157, -1.1046, -0.7982, -0.0397,\n",
       "        -1.3845,  0.2805,  0.3128,  0.2121, -0.3989,  0.2836,  1.6595, -1.7762,\n",
       "        -0.1208,  0.8577, -0.0425, -0.3098, -0.6341, -0.6159, -0.7735,  0.8514,\n",
       "        -0.6448, -0.0686,  0.1370,  0.2293, -0.0859, -2.0357,  1.0383, -1.3259,\n",
       "         0.5748, -0.0483,  0.0436, -1.6986, -1.4176, -1.2189,  0.4861,  0.4967,\n",
       "        -0.9279, -0.8002, -0.0323, -2.8358,  0.1157, -1.4193, -0.6376,  0.2255,\n",
       "         0.3400, -0.8765,  0.7401, -0.7537,  1.8084, -0.8462, -1.8286, -1.9487,\n",
       "        -0.8952, -0.5999, -1.1553, -1.7724,  0.4262, -0.7032,  0.6454, -0.5208,\n",
       "         0.6742, -0.6968,  0.2483, -0.3675, -0.8308,  0.5099, -2.0244, -0.7917,\n",
       "         0.9039,  1.0445, -0.1016, -0.1148, -1.8705, -0.0868,  0.5005, -0.9067,\n",
       "         1.4955, -1.4520, -0.2804,  0.9535, -0.6732,  0.9717, -0.0445,  0.8808,\n",
       "         2.0937, -0.5675, -1.3792, -0.6508], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Embedding(10, 100)(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 30\n",
    "N_CONCEPTS = 5\n",
    "\n",
    "query_w = nn.Embedding(N_CONCEPTS, EMBED_DIM)\n",
    "keys_w = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "\n",
    "input_embedding = torch.rand((BATCH_SIZE, SEQ_LEN, EMBED_DIM))\n",
    "keys = keys_w(input_embedding)\n",
    "values = keys_w(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 100]), torch.Size([64, 30, 100]), torch.Size([64, 30, 100]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_w.weight.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 5, 30]), torch.Size([64, 5, 100]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.matmul(query_w.weight, keys.transpose(-2, -1))\n",
    "m = F.softmax(x  / (EMBED_DIM ** (1 /2)), dim=-1)\n",
    "out = torch.matmul(m, keys)\n",
    "m.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 30])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0413, 0.0517, 0.0329, 0.0268, 0.0346, 0.0383, 0.0256, 0.0314, 0.0227,\n",
       "        0.0369, 0.0412, 0.0261, 0.0311, 0.0406, 0.0363, 0.0346, 0.0344, 0.0329,\n",
       "        0.0268, 0.0274, 0.0363, 0.0286, 0.0327, 0.0277, 0.0456, 0.0318, 0.0312,\n",
       "        0.0313, 0.0320, 0.0291], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([10, 10, 5]) / np.array([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n_tokens = np.zeros(6)\n",
    "indices_np = np.array([0, 0, 4, 5])\n",
    "# n_tokens[indices_np]+= 1\n",
    "print(n_tokens[indices_np])\n",
    "\n",
    "scores_np = np.array([1, 1, 1, 1])\n",
    "scores_np_prev = np.array([0, 0, 0, 0])\n",
    "\n",
    "# np.put(n_tokens, indices_np, scores_np + scores_np_prev)\n",
    "print(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 0., 2., 2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens[indices_np] += scores_np\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.put(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_elem, index_np in enumerate(indices_np):\n",
    "    n_tokens[index_np] += scores_np[idx_elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bottleneck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
